{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13914767,"sourceType":"datasetVersion","datasetId":8866230}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ФИКС ДЛЯ KAGGLE\n!pip install protobuf==3.20.3 --quiet\n!pip uninstall -y tensorflow tensorflow-gpu keras --quiet\n\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"USE_TF\"] = \"0\"\n# ---","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport glob\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW\n\n\nSEED = 42\nMODEL_NAME = \"cointegrated/rubert-tiny\"\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 32\nVAL_BATCH_SIZE = 64\nEPOCHS = 3\nLR = 2e-5\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n\n# Репродюсабельность\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(SEED)\n\n\ndef find_file(pattern):\n    files = glob.glob(pattern, recursive=True)\n    return files[0] if files else None\n\ntrain_path = find_file(\"/kaggle/input/**/train.csv\")\nprint(\"train_path:\", train_path)\n\nif train_path is None:\n    raise FileNotFoundError(\"Не найден train.csv. Добавь датасет через 'Add data'.\")\n\ntrain_df = pd.read_csv(train_path)\nprint(\"train_df shape:\", train_df.shape)\nprint(train_df.head())\n\n\nif \"text\" not in train_df.columns or \"label\" not in train_df.columns:\n    raise ValueError(\"train.csv должен содержать 'text' и 'label'.\")\n\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass ReviewsDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        return item\n\n\ntrain_part, val_part = train_test_split(\n    train_df,\n    test_size=0.2,\n    stratify=train_df[\"label\"],\n    random_state=SEED\n)\n\ntrain_loader = DataLoader(\n    ReviewsDataset(train_part[\"text\"].tolist(), train_part[\"label\"].tolist(), tokenizer, MAX_LEN),\n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=True\n)\nval_loader = DataLoader(\n    ReviewsDataset(val_part[\"text\"].tolist(), val_part[\"label\"].tolist(), tokenizer, MAX_LEN),\n    batch_size=VAL_BATCH_SIZE,\n    shuffle=False\n)\n\nprint(\"Train size:\", len(train_part))\nprint(\"Val size:\", len(val_part))\n\n\nnum_labels = train_df[\"label\"].nunique()\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=num_labels\n)\nmodel.to(DEVICE)\n\noptimizer = AdamW(model.parameters(), lr=LR)\ntotal_steps = len(train_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1 * total_steps),\n    num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef train_one_epoch(model, loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0.0\n\n    pbar = tqdm(loader, desc=\"Training\")\n    for batch in pbar:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        pbar.set_postfix({\"loss\": loss.item()})\n\n    return total_loss / len(loader)\n\ndef eval_model(model, loader, device):\n    model.eval()\n    preds = []\n    trues = []\n    total_loss = 0.0\n\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\")\n        for batch in pbar:\n            labels = batch[\"labels\"].to(device)\n            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n\n            outputs = model(**inputs, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            total_loss += loss.item()\n\n            pred = torch.argmax(logits, dim=1).cpu().numpy()\n            preds.extend(pred)\n            trues.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(loader)\n    f1 = f1_score(trues, preds, average=\"macro\")\n    return avg_loss, f1\n\n\nbest_f1 = 0\nbest_state_dict = None\n\nfor epoch in range(EPOCHS):\n    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n    val_loss, val_f1 = eval_model(model, val_loader, DEVICE)\n\n    print(f\"Train loss: {train_loss:.4f}\")\n    print(f\"Val loss:   {val_loss:.4f}\")\n    print(f\"Val F1:     {val_f1:.4f}\")\n\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_state_dict = model.state_dict()\n        print(\">>> New best model saved (in RAM).\")\n\nprint(\"\\nBest macro-F1:\", best_f1)\n\n\nsave_path = \"/kaggle/working/model\"\nos.makedirs(save_path, exist_ok=True)\n\nif best_state_dict is not None:\n    model.load_state_dict(best_state_dict)\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(\"Модель сохранена в:\", save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Сохранение модели\n\nimport os\nimport shutil\n\nsave_path = \"/kaggle/working/model\"\n\nprint(\"Создаю директорию:\", save_path)\nos.makedirs(save_path, exist_ok=True)\n\n# Сохраняем модель и токенизатор\nprint(\"Сохраняю модель...\")\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(\"\\nПроверяю содержимое каталога модели:\")\n!ls -lh /kaggle/working/model\n\n# Создаём ZIP, чтобы можно было скачать\nzip_path = \"/kaggle/working/model.zip\"\nprint(\"\\nАрхивирую модель в:\", zip_path)\n\n# если zip уже существовал — удалим\nif os.path.exists(zip_path):\n    os.remove(zip_path)\n\n!zip -r /kaggle/working/model.zip /kaggle/working/model > /dev/null\n\nprint(\"\\nГотово! Вот архив:\")\n!ls -lh /kaggle/working/model.zip\n\nprint(\"\\nТеперь model.zip появится справа в разделе Output — его можно скачать.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}