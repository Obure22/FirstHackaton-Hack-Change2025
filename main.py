import os
import uuid
from io import BytesIO

from flask import (
    Flask, render_template, request,
    send_file, redirect, url_for, flash
)
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import f1_score, classification_report
from tqdm import tqdm

# ==========================
#        –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================

app = Flask(__name__)
app.secret_key = "super-secret-key"
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# –£—Å–∫–æ—Ä–µ–Ω–∏—è
os.environ["TOKENIZERS_PARALLELISM"] = "true"
torch.set_grad_enabled(False)

# ==========================
#       –£–°–¢–†–û–ô–°–¢–í–û
# ==========================

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("üî• –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU")
else:
    device = torch.device("cpu")
    print("‚öôÔ∏è GPU –Ω–µ—Ç ‚Äî —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞ CPU")

# ==========================
#       –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# ==========================

MODEL_PATH = "model"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
model.to(device)
model.eval()

LABELS = {
    0: "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è",
    1: "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è",
    2: "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è"
}

# ==========================
#     –°–£–ü–ï–†-–£–°–ö–û–†–ï–ù–ù–´–ô –ò–ù–§–ï–†–ï–ù–°
# ==========================

def predict_batch(texts, batch_size=128):
    """
    –£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.
    - –ë–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏ (128)
    - max_length 128 (–≤–º–µ—Å—Ç–æ 256)
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    - –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
    """
    all_preds = []
    encoded_batches = []

    # -------- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è --------
    print("‚åõ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    for i in tqdm(range(0, len(texts), batch_size), desc="Tokenizing", ncols=80):
        batch = texts[i:i + batch_size]
        enc = tokenizer(
            batch,
            truncation=True,
            padding=True,
            max_length=128,
            return_tensors="pt"
        )
        encoded_batches.append(enc)

    # -------- –ú–û–î–ï–õ–¨ --------
    print("üî• –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏...")
    for enc in tqdm(encoded_batches, desc="Model", ncols=80):
        enc = {k: v.to(device) for k, v in enc.items()}

        logits = model(**enc).logits
        preds = logits.argmax(dim=1).cpu().numpy()
        all_preds.extend(preds)

    return all_preds


# ==========================
#           ROUTES
# ==========================

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/analyze", methods=["POST"])
def analyze():
    if 'file' not in request.files:
        flash("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return redirect(url_for("index"))

    file = request.files['file']
    if file.filename == "":
        flash("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω.")
        return redirect(url_for("index"))

    try:
        df = pd.read_csv(file)
    except Exception as e:
        flash(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
        return redirect(url_for("index"))

    if 'text' not in df.columns:
        flash("–í CSV –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∞ text.")
        return redirect(url_for("index"))

    texts = df['text'].astype(str).tolist()

    print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(texts)}")
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é...")

    preds = predict_batch(texts)

    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")

    df['pred'] = preds
    df['label_name'] = df['pred'].map(LABELS)

    if 'src' not in df.columns:
        df['src'] = 'unknown'

    counts = df['label_name'].value_counts().to_dict()

    file_id = str(uuid.uuid4())
    out_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}.csv")
    df.to_csv(out_path, index=False, encoding="utf-8")

    preview_df = df.head(200)

    return render_template(
        "results.html",
        file_id=file_id,
        table=preview_df.to_dict(orient="records"),
        counts=counts,
        labels_order=list(LABELS.values())
    )


@app.route("/download/<file_id>")
def download(file_id):
    path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_id}.csv")
    if not os.path.exists(path):
        flash("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return redirect(url_for("index"))

    return send_file(
        path,
        as_attachment=True,
        download_name="predicted.csv",
        mimetype="text/csv"
    )


@app.route("/evaluate", methods=["POST"])
def evaluate():
    if 'file' not in request.files:
        flash("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return redirect(url_for("index"))

    file = request.files['file']
    if file.filename == "":
        flash("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω.")
        return redirect(url_for("index"))

    try:
        df = pd.read_csv(file)
    except Exception as e:
        flash(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
        return redirect(url_for("index"))

    if 'text' not in df.columns or 'label' not in df.columns:
        flash("–í CSV –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å text –∏ label.")
        return redirect(url_for("index"))

    texts = df['text'].astype(str).tolist()
    true_labels = df['label'].tolist()

    print(f"üìÑ –°—Ç—Ä–æ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {len(texts)}")
    print("üöÄ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏...")

    preds = predict_batch(texts)

    macro_f1 = f1_score(true_labels, preds, average="macro")

    report = classification_report(
        true_labels,
        preds,
        target_names=[LABELS[i] for i in sorted(LABELS.keys())],
        digits=4
    )

    return render_template(
        "metrics.html",
        macro_f1=round(macro_f1, 4),
        report=report
    )


if __name__ == "__main__":
    # host="0.0.0.0" ‚Äî –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –¥–ª—è –¥–µ–ø–ª–æ—è
    app.run(debug=True)
